# Fundamentals-of-Generative-AI---Assignment-01
A neural network is like a layered team of workers. First, you have the input layer, which grabs all the raw data (like text or pixels from an image) and passes it along. Then there are hidden layers in the middle, where things get refined as they analyze and look for patterns. Finally, the output layer is where the network makes a decision, like identifying an object in an image or understanding a sentence.
=> Types of neural networks
      >-    Feedforward neural networks
      >-    Backpropagation algorithm
      >-    Convolutional neural networks
=> Training of Neural Networks 
     * Supervised learning
     * UnSupervised learning
To make things simpler, we break down the data into tokens—small chunks like words, parts of words, or even individual pixels. Breaking data into tokens, or tokenization, enables the model to focus on manageable data pieces, improving its ability to recognize patterns and relationships effectively. This process is critical for neural networks, especially in tasks like natural language processing or image recognition, where simplified data structures enhance accuracy and efficiency.
Alongside tokens, parameters are internal variables that determine the model's responses and results. During training, parameters—such as 
>-  weights     >-  biases    >-  learning rates   >-  activation functions   >- kernel sizes—are adjusted to refine model predictions. Each type of parameter serves a specific function; for instance, weights influence the importance of data inputs, while learning rates control the speed at which adjustments are made. 
Together, tokenization and parameter tuning enable neural networks, particularly deep neural networks, to analyze complex, unstructured data in tasks that demand precise pattern recognition, like identifying images or interpreting language autonomously.



